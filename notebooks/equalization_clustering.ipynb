{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "# Cluster analysis\n",
        "\n",
        "In this notebook is described the clusterization process of datasets identified in :numref:`clustering-datasets`. The goal is to find out if there is any evident difference in the audio tapes acquired with different equalizations.\n",
        "\n",
        "Looking in details, it will be tested the capacity of K-Means and hierarchical clustering to find out if:\n",
        "\n",
        "- there is an evident difference in audio tapes acquired with different equalizations for tapes recorded and reproduced at a speed of 7.5 ips;\n",
        "- there is an evident difference in tapes acquired with different equalizations for tapes recorded and reproduced at a speed of 15 ips;\n",
        "- there is a difference between audio tapes recorded and acquired at different velocities among all possibilities (3.75 ips, 7.5 ips, 15 ips) and among the most common ones (7.5, 15)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since this analysis is gonig to use the K-Means algorithm, which starts pointing to random numbers, for reproducibility purposes, the seed of the random number generator is set at $42$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "SEED = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To make sure that the best results are obtained the algorithm's parameters are going to *tuned*, but, since there is no need to cross validate the results neither to split the dataset in the training of the model, the :func:`ml.clusters.validate` function will be used. This function takes a model, a dataset, a set of parameters and a scoring function and returns the best parameters and the score obtained with them.\n",
        "\n",
        "To make a decision over the quality of a clustering result, between all possible metrics, the :py:func:`sklearn.metrics.v_measure_score` is the one that best suits our needs. It is a function that measures the homogeneity of the clusters and the completeness of the clustering (go to `cluster-scoring` for a more detailed description of *V-measure*).\n",
        "\n",
        "In the following code snippet is presented a standard workflow to tune the parameters of a clustering algorithm. The dataset used is the **A** from :numref:`clustering-datasets`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import v_measure_score\n",
        "from ml.clusters import validate\n",
        "from ml.datasets import load_dataset\n",
        "\n",
        "# load all possible combinations of 7.5 ips in CCIR and NAB\n",
        "# with noise type C from Pretto's dataset\n",
        "# e. g. ['7C_7C', '7C_7N', '7N_7C', '7N_7N']\n",
        "data, y, n_clust = load_dataset('A', noise_type='C', return_X_y=True)\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the dataset contains the noise type that is of type ``object``, it is necessary to drop it from the dataset before applying the clustering algorithm.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = data.drop('noise_type', axis=1)\n",
        "\n",
        "# define the parameters to tune\n",
        "k_params = {\n",
        "    'algorithm': ['lloyd', 'elkan'],\n",
        "    'init': ['k-means++', 'random'],\n",
        "    'max_iter': [10, 100, 300, 400],\n",
        "}\n",
        "\n",
        "parameters, score = validate(\n",
        "    KMeans(n_clusters=n_clust, n_init='auto', random_state=SEED), data, y,\n",
        "    k_params, v_measure_score)\n",
        "\n",
        "parameters, score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A good idea to automatize the process of execution the previous workflow over all possible datasets indivituated is to wrap it up in a function: ``evaluate_kmeans``. This function takes a dataset, the labels of the dataset, the number of clusters and returns the labels predicted by the model, the score obtained and the clusters centroids.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import v_measure_score\n",
        "from ml.clusters import validate\n",
        "\n",
        "\n",
        "def evaluate_kmeans(data, y, n_clusters):\n",
        "\n",
        "    k_params = {\n",
        "        'algorithm': ['lloyd', 'elkan'],\n",
        "        'init': ['k-means++', 'random'],\n",
        "        'max_iter': [10, 100, 300, 400],\n",
        "    }\n",
        "\n",
        "    parameters, score = validate(\n",
        "        KMeans(n_clusters=n_clusters, n_init='auto', random_state=SEED), data,\n",
        "        y, k_params, v_measure_score)\n",
        "\n",
        "    k_means = KMeans(n_clusters=n_clusters, n_init='auto', random_state=SEED)\n",
        "    k_means.set_params(**parameters)\n",
        "    # since we are clustering fit_predict produces\n",
        "    # the same result as fit followed by predict in this case we could also\n",
        "    # use k_means.labels_ instead of y_pred_kmeans, it is just a matter of taste\n",
        "    y_pred_kmeans = k_means.fit_predict(data)\n",
        "\n",
        "    return y_pred_kmeans, score, k_means.cluster_centers_, parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The same workflow is used to tune the parameters of the :class:`sklearn.cluster.AgglomerativeClustering` model. This time the algorithm doesn't need to set a seed because it is deterministic.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "\n",
        "def evaluate_h_cluster(data, y, n_clusters):\n",
        "\n",
        "    h_params = {\n",
        "        'linkage': ['ward', 'complete', 'average', 'single'],\n",
        "        'metric': ['manhattan', 'cosine', 'euclidean', 'l1', 'l2'],\n",
        "    }\n",
        "\n",
        "    parameters, score = validate(AgglomerativeClustering(n_clusters=n_clusters),\n",
        "                                 data, y, h_params, v_measure_score)\n",
        "\n",
        "    h_clust = AgglomerativeClustering(n_clusters=n_clusters)\n",
        "    h_clust.set_params(**parameters)\n",
        "    y_pred_hclust = h_clust.fit_predict(data, y)\n",
        "\n",
        "    return y_pred_hclust, score, parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once the tuning procedures have been defined, the next step is to apply them to all the datasets. The following code snippet shows how to do it defining a function that takes a letter and the noise type and returns the labels predicted by the models, the scores obtained and the parameters used.\n",
        "\n",
        "<div class=\"alert alert-danger\"><h4>Warning</h4><p>the output of the following code cell is visible only in html version of the docs</p></div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from ml.datasets import load_dataset\n",
        "\n",
        "\n",
        "def clusterize(dataset, noise_type=None):\n",
        "    data, n_clusters = load_dataset(dataset, noise_type=noise_type)\n",
        "    data = data.drop('noise_type', axis=1)\n",
        "\n",
        "    # evaluate the dataset with kmeans and hierarchical clustering\n",
        "    y_kmeans, k_score, centers, k_params = evaluate_kmeans(\n",
        "        data.drop('label', axis=1), data['label'], n_clusters)\n",
        "    y_hclust, h_score, h_params = evaluate_h_cluster(data.drop('label', axis=1),\n",
        "                                                     data['label'], n_clusters)\n",
        "\n",
        "    return y_kmeans, k_score, centers, k_params, y_hclust, h_score, h_params\n",
        "\n",
        "\n",
        "clustering_results = []\n",
        "\n",
        "# for each dataset and noise type\n",
        "for letter in 'ABCDEFG':\n",
        "    for noise in ['A', 'B', 'C', None]:\n",
        "        # find the best parameters for kmeans and hierarchical clustering\n",
        "        y_kmeans, k_score, centers, k_params, y_hclust, h_score, h_params = clusterize(\n",
        "            letter, noise_type=noise)\n",
        "\n",
        "        # store the results\n",
        "        clustering_results.append({\n",
        "            'dataset': letter,\n",
        "            'noise_type': noise,\n",
        "            'k_clusters': y_kmeans,\n",
        "            'k_score': k_score,\n",
        "            'k_centroids': centers,\n",
        "            'k_params': k_params,\n",
        "            'h_clusters': y_hclust,\n",
        "            'h_score': h_score,\n",
        "            'h_params': h_params\n",
        "        })\n",
        "\n",
        "dataframe = pd.DataFrame(clustering_results)\n",
        "dataframe.drop(['k_centroids', 'k_clusters', 'h_clusters'], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Last step is to plot the results. The following code snippet shows how to do it. The function ``plot_results`` takes a dataset and plots the results of the clustering algorithms. To easily visualize the plots the data is projected in a 2D space using :class:`sklearn.decomposition.PCA`, which finds the optimal projection of the data in a 2D space.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import warnings\n",
        "from sklearn.decomposition import PCA\n",
        "from ml.visualization import DATASETS_LEGEND\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "def plot_results(dataset, noise_type, k_clusters, k_score, k_centroids,\n",
        "                 k_params, h_clusters, h_score, h_params, axes):\n",
        "    # load the dataset\n",
        "    data = load_dataset(dataset, noise_type=noise_type)[0]\n",
        "\n",
        "    # project the data in a 2D space\n",
        "    transformer = PCA(n_components=2)\n",
        "    X_2 = transformer.fit_transform(data.drop(['label', 'noise_type'], axis=1))\n",
        "\n",
        "    # generate a dataframe with the projected data and the labels\n",
        "    # (numerical labels of clusters found are replaced with their semantic meaning,\n",
        "    # e.g. 0 -> 'tapes at 7.5 ips', 1 -> 'tapes at 15 ips', etc.)\n",
        "    X_2 = pd.DataFrame(X_2, columns=['x', 'y'])\n",
        "    X_2['label'] = np.array(data['label'])\n",
        "    X_2 = X_2.replace(DATASETS_LEGEND[dataset]['labels'])\n",
        "\n",
        "    # plot on the left subplot the real distribution of the data\n",
        "    sns.scatterplot(x='x',\n",
        "                    y='y',\n",
        "                    hue='label',\n",
        "                    data=X_2,\n",
        "                    ax=axes[0],\n",
        "                    palette=sns.color_palette())\n",
        "    axes[0].set_title(\n",
        "        f'Real distribution, noise type: {noise_type if noise_type is not None else \"All\"}'\n",
        "    )\n",
        "    axes[0].legend(title=DATASETS_LEGEND[dataset]['title'])\n",
        "\n",
        "    # plot on the central subplot the clusters found by kmeans\n",
        "    c = transformer.transform(k_centroids)\n",
        "    sns.scatterplot(x='x',\n",
        "                    y='y',\n",
        "                    hue=k_clusters,\n",
        "                    data=X_2,\n",
        "                    ax=axes[1],\n",
        "                    palette=sns.color_palette())\n",
        "    axes[1].get_legend().remove()\n",
        "    axes[1].set_title(f'KMeans score: {k_score:.3f}')\n",
        "    axes[1].scatter(c[:, 0], c[:, 1], marker='x', s=100, c='black')\n",
        "\n",
        "    # plot on the right subplot the clusters found by hierarchical clustering\n",
        "    sns.scatterplot(x='x',\n",
        "                    y='y',\n",
        "                    hue=h_clusters,\n",
        "                    data=X_2,\n",
        "                    ax=axes[2],\n",
        "                    palette=sns.color_palette())\n",
        "    axes[2].get_legend().remove()\n",
        "    axes[2].set_title(f'H. clustering score: {h_score:.3f}')\n",
        "\n",
        "\n",
        "# plot the results for each dataset contained in the clustering_results list\n",
        "for i in range(0, len(clustering_results), 4):\n",
        "    fig, axes = plt.subplots(4,\n",
        "                             3,\n",
        "                             figsize=(12, 16),\n",
        "                             constrained_layout=True,\n",
        "                             sharey=True,\n",
        "                             sharex=True)\n",
        "    fig.suptitle(\n",
        "        f\"Clustering results for dataset {clustering_results[i]['dataset']}\",\n",
        "        fontsize=16)\n",
        "    for j, noise in enumerate(['A', 'B', 'C', None]):\n",
        "        plot_results(**clustering_results[i + j], axes=axes[j])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "audiohandler-BO7RUw7L-py3.10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "b599be24a8d93884fd001ad20dc2e37bac390bcb7f16df6697b044b0aa8ad4fa"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
