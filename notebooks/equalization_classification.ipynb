{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Classification analysis\n\nThis notebook shows the performance of classification algorithms on the datasets from `datasets`.\n\nAs for clustering analysis, the datasets have been splitted in smaller ones to look at specific aspects of the data, but this time also a global analysis has been made (see `all_classes` and `mixin_datasets`).\n\nIn synthesis the subsets are elencated in :numref:`classification-datasets`. Since those datasets are used to train classification models, they are all subsets of Pretto dataset which comprehends more cases.\n\nTo evaluate the performance of the classification algorithms has been used the accuracy score, which is the ratio of the number of correctly classified samples to the total number of samples.\n\nThe classification is always performed tuning the hyperparameters of the model using a grid search with cross validation at 5 folds on 80% of the selected dataset, once the parameters have been tuned the model performance is tested over the remaining 20% of the data. The models taken in consideration are: K-Nearest Neighbors, Support Vector Machine, Decision Tree, Random Forest.\n\nIn the first part of the notebook datasets are analyzed separately, trained and validated on the same data splitted 80% for training and 20% for testing. The procedure to find and evaluate the best models are as follow:\n\n1. split the dataset in 80% and 20%\n2. tune and train KNN, SVM, DT and RF on the dataset without splitting by noise type\n3. for each noise type and for each model train the model on the subset of the dataset with the best parameters found previously\n4. test each model trained on a specific validation set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Firslty we set the seed for reproducibility at a specific value.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "SEED = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Workflow explanation\nLet's see the classification process over a specific dataset (*H*).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from ml.datasets import load_dataset\n\nX, y = load_dataset('H', return_X_y=True, purpose='classification')\nX = X.drop(columns=['noise_type'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We split the dataset in train and test set, reserving 20% of the data as validation set.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size=0.2,\n                                                    random_state=SEED)\nprint('Number of train Samples:', len(X_train), 'Number of test Samples:',\n      len(X_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We firstly explore the K-Nearest Neighbors algorithm.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\nfrom ml.classification import evaluate_model\n\nknn = evaluate_model(KNeighborsClassifier(),\n                     X_train,\n                     y_train,\n                     params={\n                         'n_neighbors': [x for x in range(1, 100)],\n                         'weights': ['uniform', 'distance'],\n                         'metric':\n                         ['euclidean', 'manhattan', 'chebyshev', 'minkowski']\n                     })\n\nprint(\"KNN best params:\", knn.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's see what happens in function of the parameters passed for tuning\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nfrom ml.visualization import plot_grid_search_validation_curve\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\nfor param, ax in zip(['n_neighbors', 'weights', 'metric'], axes.flatten()):\n    plot_grid_search_validation_curve(knn,\n                                      param,\n                                      ax=ax,\n                                      title=f\"{param}\",\n                                      ylim=(0.85, 1.01))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n\ny_pred = knn.predict(X_test)\naccuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The accuracy is really good, for further analysis we can look at the classification report and the confusion matrix.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from ml.visualization import plot_confusion_matrix\n\nprint(classification_report(y_test, y_pred))\nplot_confusion_matrix(y_test,\n                      y_pred,\n                      title='KNN Confusion matrix',\n                      labels=['7C_7C', '7C_7N', '7N_7C', '7N_7N'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The report still gives excellent results. The next step is to look for better results using SVM, Decision Tree and Random Forest.\n\nStarting from SVM, we approach the problem in a similar way to the previous one:\n\n- we tune the hyperparameters of the model using a grid search with cross validation at 5 folds\n- we plot the validation curve for each hyperparameter\n- we evaluate the model on the validation set\n- we look at the confusion matrix\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n\nsvm = evaluate_model(SVC(random_state=SEED),\n                     X_train,\n                     y_train,\n                     params={\n                         'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n                         'degree': [x for x in range(1, 5)],\n                         'gamma': ['scale', 'auto'],\n                     })\n\nprint(\"SVM best params:\")\nprint(svm.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\nfor param, ax in zip(['kernel', 'degree', 'gamma'], axes.flatten()):\n    plot_grid_search_validation_curve(svm,\n                                      param,\n                                      ax=ax,\n                                      title=f\"{param}\",\n                                      ylim=(0.5, 1.05))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "y_pred = svm.predict(X_test)\nprint('accuracy:', accuracy_score(y_test, y_pred))\nplot_confusion_matrix(y_test,\n                      y_pred,\n                      title='SVM Confusion matrix of dataset H',\n                      labels=['7C_7C', '7C_7N', '7N_7C', '7N_7N'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Perfect, the results are the same as in KNN.\n\nNow that a methodology has been defined we can produce a routine to train 4 models for each dataset. The function ``classify`` will return a list with all best parameters obtained from GridSearchCV for each model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom ml.datasets import load_pretto\n\n\ndef classify(dataset=None):\n\n    if dataset is None:\n        X, y = load_pretto(return_X_y=True)\n\n    else:\n        X, y = load_dataset(dataset, return_X_y=True, purpose='classification')\n\n    labels = y.unique()\n    X = X.drop(columns=['noise_type'], axis=1)\n    X_train, X_test, y_train, y_test = train_test_split(X,\n                                                        y,\n                                                        test_size=0.2,\n                                                        random_state=SEED)\n\n    knn = evaluate_model(\n        KNeighborsClassifier(),\n        X_train,\n        y_train,\n        params={\n            'n_neighbors': [x for x in range(1, 50)],\n            'weights': ['uniform', 'distance'],\n            'metric': ['euclidean', 'manhattan', 'chebyshev', 'minkowski']\n        })\n\n    svm = evaluate_model(SVC(random_state=SEED),\n                         X_train,\n                         y_train,\n                         params={\n                             'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n                             'degree': [x for x in range(1, 5)],\n                             'gamma': ['scale', 'auto'],\n                         })\n\n    tree = evaluate_model(DecisionTreeClassifier(random_state=SEED),\n                          X_train,\n                          y_train,\n                          params={\n                              'max_features': ['log2', 'sqrt', None],\n                              'criterion': ['gini', 'log_loss'],\n                              'min_samples_leaf': [1, 2, 5, 10, 20],\n                              'max_depth': [None, 2, 5, 10, 20, 30],\n                              'splitter': ['best'],\n                          })\n\n    rfc = evaluate_model(RandomForestClassifier(random_state=SEED),\n                         X_train,\n                         y_train,\n                         params={\n                             'n_estimators': [x for x in range(1, 121, 10)],\n                             'max_features': ['log2', 'sqrt'],\n                             'criterion': ['gini', 'log_loss'],\n                             'min_samples_leaf': [1, 2, 5, 10],\n                         })\n\n    models_list = [('KNN', knn), ('SVM', svm), ('Decision Tree', tree),\n                   ('Random Forest', rfc)]\n    fig, axes = plt.subplots(2, 2, figsize=(8, 7), constrained_layout=True)\n    fig.suptitle(\n        f'Confusion matrices on entire dataset {dataset if dataset is not None else \"\"}',\n        fontsize=16)\n\n    for ax, (name, model) in zip(axes.flatten(), models_list):\n\n        y_pred = model.predict(X_test)\n        acc = accuracy_score(y_test, y_pred)\n        plot_confusion_matrix(y_test,\n                              y_pred,\n                              ax=ax,\n                              title=f\"{name} - {acc}\",\n                              labels=labels)\n\n    return models_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then ``plot_performance`` will print the confusion matrix for each algorithm and noise type.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_performance(dataset, trained_models):\n    fig, axes = plt.subplots(4, 3, figsize=(11, 13), constrained_layout=True)\n    fig.suptitle(f'Confusion matrices on noise type, dataset {dataset}',\n                 fontsize=16)\n\n    for ax_line, (name, model) in zip(axes, trained_models):\n\n        for ax, noise_type in zip(ax_line.flatten(), ['A', 'B', 'C']):\n            if dataset is None:\n                X, y = load_pretto(return_X_y=True,\n                                   filters={'noise_type': noise_type})\n            else:\n                X, y = load_dataset(dataset,\n                                    noise_type,\n                                    return_X_y=True,\n                                    purpose='classification')\n            X = X.drop(columns=['noise_type'], axis=1)\n            labels = y.unique()\n            X_train, X_test, y_train, y_test = train_test_split(\n                X, y, test_size=0.2, random_state=SEED)\n            model.fit(X_train, y_train)\n            y_pred = model.predict(X_test)\n            acc = accuracy_score(y_test, y_pred)\n            plot_confusion_matrix(\n                y_test,\n                y_pred,\n                ax=ax,\n                title=f\"{name} - noise {noise_type} - {acc:.3f}\",\n                labels=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## All Pretto subsets\n\nOnce we defined two functions to evaluate the models performance let's evaluate all the datasets defined above:\n\n- subsets H, I, J, K are going to classify pre and post emphasis equalization curves\n- subset L is going to classify the tape's speed used in both recording and replaying (7.5 or 15 ips)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\nfor dataset in 'HIJKL':\n    trained_models = classify(dataset)\n    print(\n        pd.DataFrame([(name, model.best_score_, model.best_params_)\n                      for name, model in trained_models],\n                     columns=['Model', 'Best Score', 'Trained model']))\n    plot_performance(dataset, trained_models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n## All classes\n\nLet's see what happens if we use all the classes of the dataset Pretto.\nSince this dataset is greater than the others, we will exclude the SVM model from the evaluation because it takes too long to train, and the decision tree, which underperforms RF.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, y = load_pretto(return_X_y=True)\nX = X.drop(columns=['noise_type'], axis=1)\nlabels = y.unique()\nlabels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This time there are 25 classes.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size=0.2,\n                                                    random_state=SEED)\n\nknn = evaluate_model(KNeighborsClassifier(),\n                     X_train,\n                     y_train,\n                     params={\n                         'n_neighbors': [x for x in range(1, 50)],\n                         'weights': ['uniform', 'distance'],\n                         'metric':\n                         ['euclidean', 'manhattan', 'chebyshev', 'minkowski']\n                     })\n\nrfc = evaluate_model(RandomForestClassifier(random_state=SEED),\n                     X_train,\n                     y_train,\n                     params={\n                         'n_estimators': [x for x in range(1, 121, 10)],\n                         'max_features': ['log2', 'sqrt'],\n                         'criterion': ['gini', 'log_loss'],\n                         'min_samples_leaf': [1, 2, 5, 10],\n                     })\n\nmodels_list = [('KNN', knn), ('Random Forest', rfc)]\n\npd.DataFrame([(name, model.best_score_, model.best_params_)\n              for name, model in models_list],\n             columns=['Model', 'Best Score', 'Trained model'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we plot the confusion matrices:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(14, 7), constrained_layout=True)\nfig.suptitle('Confusion matrices on entire dataset', fontsize=20)\n\nfor ax, (name, model) in zip(axes, models_list):\n    y_pred = model.predict(X_test)\n    acc = accuracy_score(y_test, y_pred)\n    plot_confusion_matrix(y_test,\n                          y_pred,\n                          ax=ax,\n                          title=f\"{name} - {acc}\",\n                          labels=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's see more result stats:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for (name, model) in models_list:\n    y_pred = model.predict(X_test)\n    report = classification_report(y_test, y_pred, output_dict=True)\n    print(name)\n    print(pd.DataFrame(report).transpose())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n## Mixin datasets\nOnce the best model has been found, it's interesting to see if the model, trained on the whole dataset, is able to generalize well. To do this we can train the model on the Pretto dataset, which contains more equalization parameters, and is expected to have a better coverage over all possible inputs and test it on the Berio-Nono dataset, which is composed only by features of correctly equalized tapes.\n\nThe model selected for this analysis is the Random Forest, which gave the best results in the previous analysis.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from ml.datasets import load_berio_nono, load_pretto\nimport matplotlib.pyplot as plt\n\nX_train, y_train = load_pretto(return_X_y=True)\nX_test, y_test = load_berio_nono(return_X_y=True)\n\nX_train = X_train.drop(columns=['noise_type'])\nX_test = X_test.drop(columns=['noise_type'])\n\nlabels = y_train.unique()\n\nrfc.fit(X_train, y_train)\ny_pred = rfc.predict(X_test)\nif len(pd.Series(y_pred).unique()) != len(labels):\n    missing = set(labels) - set(pd.Series(y_pred).unique())\n\n    for i, m in enumerate(missing):\n        y_pred[i] = m\n\nfig, axes = plt.subplots(figsize=(9, 9), constrained_layout=True)\n\nplot_confusion_matrix(\n    y_test,\n    y_pred,\n    ax=axes,\n    title=f'Random Forest {accuracy_score(y_test, y_pred):.3f}',\n    labels=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since results are not so good, we can try to take in consideration the noise type and see if the results improve.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import warnings\n\nwarnings.filterwarnings('ignore')\n\nfig, axes = plt.subplots(1, 3, figsize=(21, 6))\nfig.suptitle('RF Confusion matrix (varying noise type)')\n\nfor ax, noise_type in zip(axes.flatten(), ['A', 'B', 'C']):\n    X_train, y_train = load_pretto(return_X_y=True,\n                                   filters={'noise_type': noise_type})\n    X_test, y_test = load_berio_nono(return_X_y=True,\n                                     filters={'noise_type': noise_type})\n\n    labels = y_train.unique()\n    X_train = X_train.drop(columns=['noise_type'])\n    X_test = X_test.drop(columns=['noise_type'])\n\n    rfc.fit(X_train, y_train)\n    y_pred = rfc.predict(X_test)\n\n    if len(pd.Series(y_pred).unique()) != len(labels):\n        missing = set(labels) - set(pd.Series(y_pred).unique())\n\n        for i, m in enumerate(missing):\n            y_pred[i] = m\n\n    plot_confusion_matrix(\n        y_test,\n        y_pred,\n        labels=labels,\n        title=f'{noise_type}, {accuracy_score(y_test, y_pred):.3f}',\n        ax=ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's see if the results improve taking in consideration also specific speeds.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from ml.datasets import load_dataset\n\nfig, axes = plt.subplots(2, 2, figsize=(13, 12))\nfig.suptitle('SVM Confusion matrix (varying noise type) with speed at 7.5 ips')\n\nsvc = SVC(kernel='poly', random_state=SEED, degree=4, gamma='auto')\n\nfor ax, noise_type in zip(axes.flatten(), ['A', 'B', 'C', None]):\n    X_train, y_train = load_dataset(return_X_y=True,\n                                    noise_type=noise_type,\n                                    purpose='classification',\n                                    letter='H')\n    test = load_berio_nono(filters={'noise_type': noise_type})\n    test = test[(test.label == '7C_7C') | (test.label == '7N_7N')]\n\n    X_train = X_train.drop(columns=['noise_type'], axis=1)\n    X_test = test.drop(columns=['noise_type', 'label'], axis=1)\n    y_test = test.label\n    y_test = y_test.replace({'7C_7C': 'C-C', '7N_7N': 'N-N'})\n\n    labels = y_train.unique()\n\n    svc.fit(X_train, y_train)\n    y_pred = svc.predict(X_test)\n\n    if len(pd.Series(y_pred).unique()) != len(labels):\n        missing = set(labels) - set(pd.Series(y_pred).unique())\n\n        for i, m in enumerate(missing):\n            y_pred[i] = m\n\n    plot_confusion_matrix(\n        y_test,\n        y_pred,\n        labels=labels,\n        title=f'{noise_type}, {accuracy_score(y_test, y_pred):.3f}',\n        ax=ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the following section we take in consideration only ``7C_7C`` and ``7N_7N`` from Pretto dataset to see how a model trained on these data performs at classifying Berio-Nono istances with this equalization parameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from ml.datasets import load_dataset\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\nfig.suptitle(\n    'SVM Confusion matrix (varying noise type) with speed at 7.5 ips and correct equalization'\n)\n\nfor ax, noise_type in zip(axes.flatten(), ['A', 'B', 'C', None]):\n    X_train, y_train = load_dataset(noise_type=noise_type,\n                                    return_X_y=True,\n                                    purpose='classification',\n                                    letter='J')\n    test = load_dataset(letter='F',\n                        noise_type=noise_type,\n                        purpose='classification')\n    test = test[(test.label == '7C_7C') | (test.label == '7N_7N')]\n\n    X_train = X_train.drop(columns=['noise_type'], axis=1)\n    X_test = test.drop(columns=['noise_type', 'label'], axis=1)\n    y_test = test.label\n    y_test = y_test.replace({'7C_7C': 'CCIR', '7N_7N': 'NAB'})\n    labels = y_train.unique()\n\n    svc.fit(X_train, y_train)\n    y_pred = svc.predict(X_test)\n\n    plot_confusion_matrix(\n        y_test,\n        y_pred,\n        labels=labels,\n        title=\n        f'{\"All\" if noise_type is None else noise_type}, {accuracy_score(y_test, y_pred):.3f}',\n        ax=ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the following section we take in consideration only ``15C_15C`` and ``15N_15N`` from Pretto dataset to see how a model trained on these data performs at classifying Berio-Nono istances with this equalization parameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from ml.datasets import load_dataset\n\nfig, axes = plt.subplots(2, 2, figsize=(13, 10))\nfig.suptitle(\n    'SVM Confusion matrix (varying noise type) with speed at 15 ips and correct equalization'\n)\n\nfor ax, noise_type in zip(axes.flatten(), ['A', 'B', 'C', None]):\n    X_train, y_train = load_dataset(noise_type=noise_type,\n                                    return_X_y=True,\n                                    purpose='classification',\n                                    letter='K')\n    test = load_dataset(letter='F',\n                        noise_type=noise_type,\n                        purpose='classification')\n    test = test[(test.label == '15C_15C') | (test.label == '15N_15N')]\n\n    X_train = X_train.drop(columns=['noise_type'], axis=1)\n    X_test = test.drop(columns=['noise_type', 'label'], axis=1)\n    y_test = test.label\n    y_test = y_test.replace({'15C_15C': 'CCIR', '15N_15N': 'NAB'})\n    labels = y_train.unique()\n\n    svc.fit(X_train, y_train)\n    y_pred = svc.predict(X_test)\n    plot_confusion_matrix(\n        y_test,\n        y_pred,\n        labels=labels,\n        title=\n        f'{\"All\" if noise_type is None else noise_type}, {accuracy_score(y_test, y_pred):.3f}',\n        ax=ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Even if the results are not so good, it is interesting to note that a model trained an validated on both datasets (Pretto and Berio-Nono) gives better results than a model trained and validated only on Pretto dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from ml.datasets import load_pretto, load_berio_nono\nfrom ml.visualization import plot_confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom ml.classification import evaluate_model\n\nSEED = 42\ndata1 = load_pretto()\ndata2 = load_berio_nono()\n\ndata = pd.concat([data1, data2])\nX = data.drop(columns=['noise_type', 'label'], axis=1)\ny = data.label\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nrfc = evaluate_model(RandomForestClassifier(random_state=SEED),\n                     X_train,\n                     y_train,\n                     params={\n                         'n_estimators': [x for x in range(90, 121, 10)],\n                         'max_features': ['log2'],\n                         'criterion': ['log_loss'],\n                         'min_samples_leaf': [1],\n                     })\ny_pred = rfc.predict(X_test)\n\nfig, ax = plt.subplots(figsize=(10, 8))\nplot_confusion_matrix(y_test,\n                      y_pred,\n                      labels=y_train.unique(),\n                      title=f'All, {accuracy_score(y_test, y_pred):.3f}',\n                      ax=ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Speed\n\nAnother important classification concerns the speed recognition. Here in the same way as above we train a model over Pretto dataset generatin a new label for speed (7.5 or 15 ips) and see if the model recognizes those speed in Berio-Nono data.\nIn particular we filter Pretto dataset to get only instances where:\n- speed was only 7.5 or 15 ips for both writing and reading operations\n- equalization curves are the same for pre and post emphasis\n(so we get only the following list of labels: ``['7C_7C', '7N_7N', '15C_15C', '15N_15N']``)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n\nfig, axes = plt.subplots(2, 2, figsize=(8, 7), constrained_layout=True)\nfig.suptitle(\n    'RF Confusion matrix (varying noise type) for speed classification')\n\nfor ax, noise_type in zip(axes.flatten(), ['A', 'B', 'C', None]):\n    train = load_pretto(filters={'noise_type': noise_type})\n    train = train[(train.label == '15C_15C') | (train.label == '15N_15N') |\n                  (train.label == '7C_7C') | (train.label == '7N_7N')]\n    test = load_berio_nono(filters={'noise_type': noise_type})\n    mapping = {'7C_7C': \"7.5 ips\", '7N_7N': \"7.5 ips\", '15C_15C': \"15 ips\", '15N_15N': \"15 ips\"}\n    train = train.replace(mapping)\n    test = test.replace(mapping)\n\n    X_train = train.drop(columns=['noise_type', 'label'], axis=1)\n    y_train = train.label\n    X_test = test.drop(columns=['noise_type', 'label'], axis=1)\n    y_test = test.label\n\n    rfc = evaluate_model(RandomForestClassifier(random_state=SEED),\n                         X_train,\n                         y_train,\n                         params={\n                             'n_estimators': [x for x in range(90, 121, 10)],\n                             'max_features': ['log2'],\n                             'criterion': ['log_loss'],\n                             'min_samples_leaf': [1],\n                         })\n    y_pred = rfc.predict(X_test)\n\n    rfc.fit(X_train, y_train)\n    y_pred = rfc.predict(X_test)\n    plot_confusion_matrix(\n        y_test,\n        y_pred,\n        labels=y_train.unique(),\n        title=\n        f'{\"All\" if noise_type is None else noise_type}, {accuracy_score(y_test, y_pred):.3f}',\n        ax=ax)\n    print(f\"Classification report for {noise_type or 'All noise types'}\")\n    print(classification_report(y_test, y_pred, target_names=y_train.unique()))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}